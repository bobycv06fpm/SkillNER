{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import spacy\n",
    "\n",
    "import oauth2client.file, oauth2client.tools\n",
    "from oauth2client import client\n",
    "import gspread\n",
    "\n",
    "client_id = '403167867560-kgeqncsauc7lht11sfkjk1klup9v06oa.apps.googleusercontent.com'\n",
    "client_secret = 'vgoDh7To1xrDHYqb3wc5Lssr'\n",
    "\n",
    "flow = client.OAuth2WebServerFlow(client_id, client_secret, 'https://spreadsheets.google.com/feeds')\n",
    "storage = oauth2client.file.Storage('credentials.dat')\n",
    "credentials = storage.get()\n",
    "if credentials is None or credentials.invalid:\n",
    "    import argparse\n",
    "    flags = argparse.ArgumentParser(parents=[oauth2client.tools.argparser]).parse_args([])\n",
    "    credentials = oauth2client.tools.run_flow(flow, storage, flags)\n",
    "\n",
    "gc = gspread.authorize(credentials)\n",
    "\n",
    "from df2gspread import df2gspread as d2g\n",
    "from df2gspread import gspread2df as g2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82438d3471d1451184e6d895debd2ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87baef8eb0046d6987b4e36b67948c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Losses {'ner': 6384.86925319556}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a87a6296d1c4ae490c72012d0120962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Losses {'ner': 2880.2105161005484}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240d2f92173742259b9c37a818fd7e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Losses {'ner': 2239.718901356168}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78aee515466f465f84db343647c3ce05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Losses {'ner': 2051.1399134368653}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04077758a1ba43658be0bff1dac92c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Losses {'ner': 1886.7062138716801}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66448c20b634ed8b227d751bb55da7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Losses {'ner': 1566.179482715063}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c78b73995b14eea9dca155102f03481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(0)\n",
    "nlp = spacy.load(\"en_core_web_sm\",disable = ['ner'])  # create blank Language class\n",
    "# Add entity recognizer to model if it's not in the pipeline\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "\n",
    "\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Only run nlp.make_doc to speed things up\n",
    "def add_phraseMatcher_ent(matcher, doc, i, matches):\n",
    "    # Get the current match and create tuple of entity label, start and end.\n",
    "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label = \"SOFT SKILL\")\n",
    "    try: \n",
    "        doc.ents += (entity,)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "patterns = [nlp.make_doc(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", add_phraseMatcher_ent, *patterns)\n",
    "nlp.to_disk(\"soft_skills\")\n",
    "\n",
    "ner.add_label('SOFT SKILL')\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "move_names = list(ner.move_names)\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    sizes = compounding(1.0, 4.0, 1.001)\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    for itn in tqdm(range(25)):\n",
    "        random.shuffle(TRAIN_)\n",
    "        batches = minibatch(TRAIN_, size=sizes)\n",
    "        losses = {}\n",
    "        for batch in tqdm(batches):\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            \n",
    "        nlp.to_disk(\"soft_skills\")\n",
    "        print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in 'problem solving'\n",
      "SOFT SKILL problem solving\n"
     ]
    }
   ],
   "source": [
    "test_text = \"problem solving\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Entities in '%s'\" % test_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROPN', 'PROPN', 'AUX', 'DET', 'NOUN', 'ADP', 'PROPN']"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t.pos_ for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the Lexicon from the web\n",
    "lexicon['soft_skill'] = lexicon['soft_skill'].apply(lambda x: x[1:] if x.startswith(' ') else x)\n",
    "lexicon['soft_skill'] = lexicon['soft_skill'].apply(lambda x: x[:-1] if x.endswith(' ') else x)\n",
    "terms = lexicon['soft_skill'].unique().tolist()\n",
    "terms.append('problem-solving')\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner'])\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Only run nlp.make_doc to speed things up\n",
    "def add_phraseMatcher_ent(matcher, doc, i, matches):\n",
    "    # Get the current match and create tuple of entity label, start and end.\n",
    "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label = \"SOFT SKILL\")\n",
    "    try: \n",
    "        doc.ents += (entity,)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "patterns = [nlp.make_doc(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", add_phraseMatcher_ent, *patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = g2d.download('1IbmUHYiQ803h2nCDfWI5EQBD6K8UUwdz7gETMdRvtrE',wks_name='Abstracts',credentials = credentials, col_names=True)\n",
    "df = df[['docId','abstract']]\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77923696596e45f79b1d82432234d443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10519.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pickling...\n"
     ]
    }
   ],
   "source": [
    "def as_doc(x):\n",
    "    \"\"\"Transform a cell string of a dataframe in a spacy_doc\"\"\"\n",
    "    \n",
    "    doc = nlp(x)\n",
    "    matcher(doc)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "#if os.path.exists('./data/interim/cv_database_spacy.pickle'):\n",
    "#    df = pd.read_pickle('./data/interim/cv_database_spacy.pickle')\n",
    "#    print('File exists.')\n",
    "#df = pd.read_excel('./data/interim/cv_database.xlsx')\n",
    "df['abstract_doc'] = df.progress_apply(lambda x: as_doc(x['abstract']),axis = 1)\n",
    "print('Pickling...')\n",
    "df.to_pickle('corpus.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cda4d4276224a9b90c5a5365c25ee93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10519.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TRAIN = []\n",
    "\n",
    "for _,row in tqdm(df.iterrows(), total = len(df)):\n",
    "    doc = row['abstract_doc']\n",
    "    sents = [s.as_doc() for s in doc.sents]\n",
    "    \n",
    "    for sent in sents:\n",
    "        \n",
    "        try:\n",
    "            doc = sent.to_json()\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        text = doc['text']\n",
    "        ents = {}\n",
    "        ents['entities'] = []\n",
    "        \n",
    "        if 'ents' not in doc.keys():\n",
    "            continue\n",
    "        \n",
    "        for ent in doc['ents']:\n",
    "            ents['entities'].append((ent['start'],ent['end'],ent['label']))\n",
    "        TRAIN.append((text,ents))\n",
    "#TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080cb38d93854c40b9a5e7305fb61016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12597.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TRAIN_ = TRAIN\n",
    "\n",
    "clues = ['able to', 'able in','ability to','ability in','capability to','capable of','know-how of','know-how in','level of','knowledge of','experience in','experience of']\n",
    "train_terms = []\n",
    "for term in terms:\n",
    "    train_terms.append(term)\n",
    "    for clue in clues:\n",
    "        train_terms.append(clue + ' ' + term)\n",
    "        \n",
    "for skill in tqdm(train_terms, total = len(train_terms)):\n",
    "    text = skill\n",
    "    ents = {}\n",
    "    ents['entities'] = [(0,len(skill),'SOFT SKILL')]\n",
    "    TRAIN_.append((text,ents))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "spacy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
